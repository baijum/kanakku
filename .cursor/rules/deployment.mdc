---
description: Production deployment standards for server management, security, and monitoring
globs: "docker-compose.yml", "Dockerfile", "**/*.{sh,yml,yaml}"
alwaysApply: true
---
# Deployment Standards

## Server Management

- Kill all related running servers before starting a new one
- Always start a new server after making changes to allow for testing

## Environment Configuration

- Use separate environment files for different stages (development, staging, production)
- Never commit production secrets to version control - use environment variables
- Generate strong, unique secret keys for each environment using `openssl rand -hex 32`
- Use different database credentials and URLs for each environment
- Production deployments must use HTTPS. Configure SSL/TLS using certificates from a trusted Certificate Authority (CA) (e.g., Let's Encrypt via Certbot). Ensure Nginx (or other reverse proxy/load balancer) is configured to:
    - Disable outdated protocols (SSLv2, SSLv3, TLS 1.0, TLS 1.1).
    - Use strong, modern cipher suites (refer to Mozilla SSL Configuration Generator for recommendations).
    - Aim for an 'A' or 'A+' rating on SSL Labs tests.
    - Implement HTTP Strict Transport Security (HSTS) headers.
- **Environment Variable Naming Convention**: Use a consistent, prefixed naming convention for `kanakku`-specific environment variables, e.g., `KANAKKU_DATABASE_URL`, `KANAKKU_REDIS_URL`, `KANAKKU_SECRET_KEY`. Document all required environment variables in the root `README.md` or a dedicated deployment guide.
- **Containerization Strategy**: `kanakku` utilizes Docker for containerization.
    - A `Dockerfile` is present for building the backend Flask application image.
    - A `Dockerfile` is present for building the frontend React application (typically a multi-stage build producing static assets served by Nginx).
    - A `docker-compose.yml` file is used for local development and testing environments to orchestrate the application, database (PostgreSQL), and other services (Redis).
    - Production deployments are managed via [e.g., Kubernetes manifests, Terraform scripts, specific cloud provider service like ECS/AppRunner - *specify if known*].
- **CI/CD Tooling**: The CI/CD pipeline for `kanakku` is implemented using [e.g., GitHub Actions, GitLab CI, Jenkins - *specify if known*]. Workflows are defined in [e.g., `.github/workflows/`, `.gitlab-ci.yml`].
- When generating or modifying configuration files for `kanakku` (e.g., `Dockerfile`, `docker-compose.yml`, Nginx config snippets, Gunicorn startup scripts, CI/CD workflow files):
    1.  Base them heavily on existing, working configurations within the `kanakku` project.
    2.  If creating anew, use widely accepted best practices and official documentation for the specific technologies (Flask, React, Nginx, Docker, PostgreSQL, Redis).
    3.  Do not invent novel or overly complex configuration approaches without strong justification.
- Ensure all file paths in `Dockerfiles` and `docker-compose.yml` are correct relative to the build context and application's working directory. Port mappings (`EXPOSE` in Dockerfile, `ports` in `docker-compose.yml`) must match the ports used by the `kanakku` application services (e.g., Gunicorn, Nginx serving frontend).
- When referencing environment variables in deployment scripts or container configurations, use the exact names as defined and expected by the `kanakku` application (see `config.py` and `KANAKKU_*` naming convention).

## Docker Best Practices

- Use multi-stage builds to minimize image size and improve security
- Run containers as non-root users for security
- Use specific version tags instead of `latest` for base images
- Implement proper health checks for all services
- Use Docker volumes for persistent data storage

## Database Deployment

- Use PostgreSQL for production instead of SQLite
- Implement automated database migrations using Alembic
- Set up regular automated backups with encryption. Automated daily backups of the PostgreSQL database are configured [e.g., via pg_dump scheduled cron job, cloud provider's managed database backup service]. Backups must be encrypted and stored securely, preferably in a separate geographical location. Test backup restoration procedures at least quarterly.
- Use connection pooling for better performance
- Monitor database performance and slow queries

## Web Server Configuration

- Use Nginx as reverse proxy for production deployments
- Implement proper SSL/TLS termination with strong cipher suites (see "Environment Configuration" for details)
- Configure security headers (HSTS, CSP, X-Frame-Options, X-Content-Type-Options)
- Configure Nginx (or CDN) to serve static assets (JS, CSS, images, fonts) for the `kanakku` frontend with appropriate caching headers:
    - Use long `Cache-Control: public, max-age=<large_value>, immutable` headers for versioned/fingerprinted assets.
    - Use `ETag` headers for cache validation for other static assets.
- Implement rate limiting to prevent abuse

## Application Server

- Use Gunicorn with multiple workers for Flask applications
- For Gunicorn, configure the number of synchronous worker processes. A common starting point is `(2 * number_of_cpu_cores) + 1`. Adjust based on application characteristics (CPU-bound vs. I/O-bound) and performance monitoring under load. For async workers (like `gevent` or `uvicorn` workers with FastAPI/Starlette if used), the calculation differs.
- Implement graceful shutdowns and restarts
- Configure Gunicorn and Flask application logs to output to `stdout`/`stderr`. This allows log collection by container orchestration systems (e.g., Docker, Kubernetes) or log shipper agents. If logging to files directly (less common in containerized environments), implement log rotation using tools like `logrotate` to prevent disk space exhaustion.
- Monitor application performance and memory usage

## Monitoring & Logging

- Implement centralized logging for all services
- Set up application performance monitoring (APM)
- Implement health check endpoints (e.g., `/api/v1/health` for the backend API, specific health endpoints for other services like Redis if applicable). A backend health check should:
    - Verify critical dependencies like database connectivity (e.g., perform a simple query like `SELECT 1`).
    - Return a `200 OK` status and a simple JSON body (e.g., `{\"status\": \"healthy\"}`) if all checks pass.
    - Return a `503 Service Unavailable` if any critical dependency check fails.
  These endpoints are used by orchestration tools (e.g., Kubernetes, Docker Swarm) for liveness and readiness probes.
- Implement alerting for critical failures and performance issues
- Monitor resource usage (CPU, memory, disk, network)

## Security Hardening

- Use firewalls to restrict network access
- Implement proper user permissions and access controls
- Regular security updates for all system packages
- Use secrets management for sensitive configuration
- Implement intrusion detection and monitoring

## Backup & Recovery

- Automated daily backups of database and application data (see "Database Deployment")
- Test backup restoration procedures regularly
- Store backups in multiple locations (local and cloud)
- Implement point-in-time recovery capabilities
- Document disaster recovery procedures

## CI/CD Pipeline

- Implement automated testing in CI pipeline
- Use linting and code quality checks
- Implement security scanning for dependencies
- Use blue-green or rolling deployments for zero downtime
- The CI/CD pipeline should support quick and reliable rollback to a previously known stable deployment version in case of failures. Document manual rollback steps for emergencies if automated rollback is not fully comprehensive.

## Performance Optimization

- Use CDN for static asset delivery
- Implement proper caching strategies (Redis, application-level)
- Optimize database queries and add appropriate indexes
- Use compression for HTTP responses
- Monitor and optimize application startup times

## Scalability Considerations

- Design for horizontal scaling from the beginning
- Use load balancers for distributing traffic
- Implement session storage that works across multiple instances
- Use message queues for background processing
- Plan for database scaling (read replicas, sharding)

## Documentation Requirements

- Maintain up-to-date deployment documentation
- Document all environment variables and their purposes
- Create runbooks for common operational tasks
- Document troubleshooting procedures
- Keep architecture diagrams current
